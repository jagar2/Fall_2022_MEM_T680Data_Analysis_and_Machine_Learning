{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "# Transforms\n",
        "\n",
        "Data does not always come in its final processed form that is required for\n",
        "training machine learning algorithms. We use **transforms** to perform some\n",
        "manipulation of the data and make it suitable for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "All TorchVision datasets have two parameters -`transform` to modify the features and\n",
        "`target_transform` to modify the labels - that accept callables containing the transformation logic.\n",
        "The [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html) module offers\n",
        "several commonly-used transforms out of the box.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "The FashionMNIST features are in PIL Image format, and the labels are integers.\n",
        "For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.\n",
        "To make these transformations, we use `ToTensor` and `Lambda`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(\n",
        "        lambda y: torch.zeros(10, dtype=torch.float).scatter_(\n",
        "            0, torch.tensor(y), value=1\n",
        "        )\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## ToTensor()\n",
        "\n",
        "[ToTensor](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor)\n",
        "converts a PIL image or NumPy `ndarray` into a `FloatTensor`. and scales\n",
        "the image's pixel intensity values in the range [0., 1.]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "source": [
        "## Lambda Transforms\n",
        "\n",
        "Lambda transforms apply any user-defined lambda function. Here, we define a function\n",
        "to turn the integer into a one-hot encoded tensor.\n",
        "It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls\n",
        "[scatter\\_](https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html) which assigns a\n",
        "`value=1` on the index as given by the label `y`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "outputs": [],
      "source": [
        "target_transform = Lambda(\n",
        "    lambda y: torch.zeros(10, dtype=torch.float).scatter_(\n",
        "        dim=0, index=torch.tensor(y), value=1\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "### Further Reading\n",
        "\n",
        "- [torchvision.transforms API](https://pytorch.org/vision/stable/transforms.html)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.0 ('jupyterbook')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "c475b5beda6d617ffb7b2fcf453fbe132321ffc1e1f96c06cf49356e1e7f42cb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
