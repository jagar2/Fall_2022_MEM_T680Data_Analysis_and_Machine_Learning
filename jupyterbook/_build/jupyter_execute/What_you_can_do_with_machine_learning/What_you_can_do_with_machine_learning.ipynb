{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a29535f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What You Can Do With Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb549cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Computer Vision and Graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6c400f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Image Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101c83f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The most common machine learning task is to take an image and classify if a single object is in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec81512",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) - The MNIST database of handwritten digits. \n",
    "\n",
    "[CAL 101](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) - Caltech-101 consists of pictures of objects belonging to 101 classes, plus one background clutter class. Each image is labelled with a single object. Each class contains roughly 40 to 800 images, totalling around 9k images. Images are of variable sizes, with typical edge lengths of 200-300 pixels. This version contains image-level labels only. The original dataset also contains bounding boxes.\n",
    "\n",
    "[ImageNet](https://www.image-net.org) -ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. The project has been instrumental in advancing computer vision and deep learning research. The data is available for free to researchers for non-commercial use.\n",
    "\n",
    "[CFAR 10](https://www.cs.toronto.edu/~kriz/cifar.html) - The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedb3b06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./figs/Classification.png)\n",
    "\n",
    "from Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NeuroIPS, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf38975",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Microsoft (Deep Residual Learning) [[Paper](http://arxiv.org/pdf/1512.03385v1.pdf)][[Slide](http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf)]\n",
    "  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition, arXiv:1512.03385.\n",
    "* Microsoft (PReLu/Weight Initialization) [[Paper]](http://arxiv.org/pdf/1502.01852)\n",
    "  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, arXiv:1502.01852.\n",
    "* Batch Normalization [[Paper]](http://arxiv.org/pdf/1502.03167)\n",
    "  * Sergey Ioffe, Christian Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, arXiv:1502.03167.\n",
    "* GoogLeNet [[Paper]](http://arxiv.org/pdf/1409.4842)\n",
    "  * Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR, 2015.\n",
    "* VGG-Net [[Web]](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) [[Paper]](http://arxiv.org/pdf/1409.1556)\n",
    "  * Karen Simonyan and Andrew Zisserman, Very Deep Convolutional Networks for Large-Scale Visual Recognition, ICLR, 2015.\n",
    "* AlexNet [[Paper]](http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012)\n",
    "  * Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369ceac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Classification Model Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb64b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/imagenet_benchmarks.jpg)\n",
    "[ImageNet Leaderboard](https://paperswithcode.com/sota/image-classification-on-imagenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f03b2cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Object Detection\n",
    "Models that try to find and detect the location of multiple objects in an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad755830",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/object_detection.png)\n",
    "\n",
    "from Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f5d9be",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "[COCO](http://cocodataset.org/#home) - COCO is a large-scale object detection, segmentation, and captioning dataset.\n",
    " \n",
    "\n",
    "[open_images_v4](https://storage.googleapis.com/openimages/web/index.html) - Open Images is a dataset of ~9M images that have been annotated with image-level labels and object bounding boxes.\n",
    "\n",
    "The training set of V4 contains 14.6M bounding boxes for 600 object classes on 1.74M images, making it the largest existing dataset with object location annotations. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (8.4 per image on average). Moreover, the dataset is annotated with image-level labels spanning thousands of classes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65741b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* PVANET [[Paper]](https://arxiv.org/pdf/1608.08021) [[Code]](https://github.com/sanghoon/pva-faster-rcnn)\n",
    "  * Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park, PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection, arXiv:1608.08021\n",
    "* OverFeat, NYU [[Paper]](http://arxiv.org/pdf/1312.6229.pdf)\n",
    "  * OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, ICLR, 2014.\n",
    "* R-CNN, UC Berkeley [[Paper-CVPR14]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) [[Paper-arXiv14]](http://arxiv.org/pdf/1311.2524)\n",
    "  * Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, CVPR, 2014.\n",
    "* SPP, Microsoft Research [[Paper]](http://arxiv.org/pdf/1406.4729)\n",
    "  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, ECCV, 2014.\n",
    "* Fast R-CNN, Microsoft Research [[Paper]](http://arxiv.org/pdf/1504.08083)\n",
    "  * Ross Girshick, Fast R-CNN, arXiv:1504.08083.\n",
    "* Faster R-CNN, Microsoft Research [[Paper]](http://arxiv.org/pdf/1506.01497)\n",
    "  * Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497.\n",
    "* R-CNN minus R, Oxford [[Paper]](http://arxiv.org/pdf/1506.06981)\n",
    "  * Karel Lenc, Andrea Vedaldi, R-CNN minus R, arXiv:1506.06981.\n",
    "* End-to-end people detection in crowded scenes [[Paper]](http://arxiv.org/abs/1506.04878)\n",
    "  * Russell Stewart, Mykhaylo Andriluka, End-to-end people detection in crowded scenes, arXiv:1506.04878.\n",
    "* You Only Look Once: Unified, Real-Time Object Detection [[Paper]](http://arxiv.org/abs/1506.02640), [[Paper Version 2]](https://arxiv.org/abs/1612.08242), [[C Code]](https://github.com/pjreddie/darknet), [[Tensorflow Code]](https://github.com/thtrieu/darkflow)\n",
    "  * Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, You Only Look Once: Unified, Real-Time Object Detection, arXiv:1506.02640\n",
    "  * Joseph Redmon, Ali Farhadi (Version 2)\n",
    "* Inside-Outside Net [[Paper]](http://arxiv.org/abs/1512.04143)\n",
    "  * Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick, Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks\n",
    "* Deep Residual Network (Current State-of-the-Art) [[Paper]](http://arxiv.org/abs/1512.03385)\n",
    "  * Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition\n",
    "* Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning [[Paper](http://arxiv.org/pdf/1503.00949.pdf)]\n",
    "* R-FCN [[Paper]](https://arxiv.org/abs/1605.06409) [[Code]](https://github.com/daijifeng001/R-FCN)\n",
    "  * Jifeng Dai, Yi Li, Kaiming He, Jian Sun, R-FCN: Object Detection via Region-based Fully Convolutional Networks\n",
    "* SSD [[Paper]](https://arxiv.org/pdf/1512.02325v2.pdf) [[Code]](https://github.com/weiliu89/caffe/tree/ssd)\n",
    "  * Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, SSD: Single Shot MultiBox Detector, arXiv:1512.02325\n",
    "* Speed/accuracy trade-offs for modern convolutional object detectors [[Paper]](https://arxiv.org/pdf/1611.10012v1.pdf)\n",
    "  * Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy, Google Research, arXiv:1611.10012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80872691",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Semantic Segmentation\n",
    "The task of clustering parts of an image together which belong to the same object class. It is a form of pixel-level prediction because each pixel in an image is classified according to a category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a808ee7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/semantic_segmentation.png)\n",
    "\n",
    "from Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e859a4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "\n",
    "[PASCAL VOC (PASCAL Visual Object Classes Challenge)](http://host.robots.ox.ac.uk/pascal/VOC/) - The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb19b3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* SEC: Seed, Expand and Constrain\n",
    "  *  Alexander Kolesnikov, Christoph Lampert, Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation, ECCV, 2016. [[Paper]](http://pub.ist.ac.at/~akolesnikov/files/ECCV2016/main.pdf) [[Code]](https://github.com/kolesman/SEC)\n",
    "* Adelaide\n",
    "  * Guosheng Lin, Chunhua Shen, Ian Reid, Anton van dan Hengel, Efficient piecewise training of deep structured models for semantic segmentation, arXiv:1504.01013. [[Paper]](http://arxiv.org/pdf/1504.01013) (1st ranked in VOC2012)\n",
    "  * Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel, Deeply Learning the Messages in Message Passing Inference, arXiv:1508.02108. [[Paper]](http://arxiv.org/pdf/1506.02108) (4th ranked in VOC2012)\n",
    "* Deep Parsing Network (DPN)\n",
    "  * Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang, Semantic Image Segmentation via Deep Parsing Network, arXiv:1509.02634 / ICCV 2015 [[Paper]](http://arxiv.org/pdf/1509.02634.pdf) (2nd ranked in VOC 2012)\n",
    "* CentraleSuperBoundaries, INRIA [[Paper]](http://arxiv.org/pdf/1511.07386)\n",
    "  * Iasonas Kokkinos, Surpassing Humans in Boundary Detection using Deep Learning, arXiv:1411.07386 (4th ranked in VOC 2012)\n",
    "* BoxSup [[Paper]](http://arxiv.org/pdf/1503.01640)\n",
    "  * Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640. (6th ranked in VOC2012)\n",
    "* POSTECH\n",
    "  * Hyeonwoo Noh, Seunghoon Hong, Bohyung Han, Learning Deconvolution Network for Semantic Segmentation, arXiv:1505.04366. [[Paper]](http://arxiv.org/pdf/1505.04366) (7th ranked in VOC2012)\n",
    "  * Seunghoon Hong, Hyeonwoo Noh, Bohyung Han, Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation, arXiv:1506.04924. [[Paper]](http://arxiv.org/pdf/1506.04924)\n",
    "  * Seunghoon Hong,Junhyuk Oh,\tBohyung Han, and\tHonglak Lee, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, arXiv:1512.07928 [[Paper](http://arxiv.org/pdf/1512.07928.pdf)] [[Project Page](http://cvlab.postech.ac.kr/research/transfernet/)]\n",
    "* Conditional Random Fields as Recurrent Neural Networks [[Paper]](http://arxiv.org/pdf/1502.03240)\n",
    "  * Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr, Conditional Random Fields as Recurrent Neural Networks, arXiv:1502.03240. (8th ranked in VOC2012)\n",
    "* DeepLab\n",
    "  * Liang-Chieh Chen, George Papandreou, Kevin Murphy, Alan L. Yuille, Weakly-and semi-supervised learning of a DCNN for semantic image segmentation, arXiv:1502.02734. [[Paper]](http://arxiv.org/pdf/1502.02734) (9th ranked in VOC2012)\n",
    "* Zoom-out [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf)\n",
    "  * Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich, Feedforward Semantic Segmentation With Zoom-Out Features, CVPR, 2015\n",
    "* Joint Calibration [[Paper]](http://arxiv.org/pdf/1507.01581)\n",
    "  * Holger Caesar, Jasper Uijlings, Vittorio Ferrari, Joint Calibration for Semantic Segmentation, arXiv:1507.01581.\n",
    "* Fully Convolutional Networks for Semantic Segmentation [[Paper-CVPR15]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf) [[Paper-arXiv15]](http://arxiv.org/pdf/1411.4038)\n",
    "  * Jonathan Long, Evan Shelhamer, Trevor Darrell, Fully Convolutional Networks for Semantic Segmentation, CVPR, 2015.\n",
    "* Hypercolumn [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hariharan_Hypercolumns_for_Object_2015_CVPR_paper.pdf)\n",
    "  * Bharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik, Hypercolumns for Object Segmentation and Fine-Grained Localization, CVPR, 2015.\n",
    "* Deep Hierarchical Parsing\n",
    "  * Abhishek Sharma, Oncel Tuzel, David W. Jacobs, Deep Hierarchical Parsing for Semantic Segmentation, CVPR, 2015. [[Paper]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Sharma_Deep_Hierarchical_Parsing_2015_CVPR_paper.pdf)\n",
    "* Learning Hierarchical Features for Scene Labeling [[Paper-ICML12]](http://yann.lecun.com/exdb/publis/pdf/farabet-icml-12.pdf) [[Paper-PAMI13]](http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf)\n",
    "  * Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers, ICML, 2012.\n",
    "  * Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Learning Hierarchical Features for Scene Labeling, PAMI, 2013.\n",
    "* University of Cambridge [[Web]](http://mi.eng.cam.ac.uk/projects/segnet/)\n",
    "  * Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla \"SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.\" arXiv preprint arXiv:1511.00561, 2015. [[Paper]](http://arxiv.org/abs/1511.00561)\n",
    "* Alex Kendall, Vijay Badrinarayanan and Roberto Cipolla \"Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.\" arXiv preprint arXiv:1511.02680, 2015. [[Paper]](http://arxiv.org/abs/1511.00561)\n",
    "* Princeton\n",
    "  * Fisher Yu, Vladlen Koltun, \"Multi-Scale Context Aggregation by Dilated Convolutions\", ICLR 2016, [[Paper](http://arxiv.org/pdf/1511.07122v2.pdf)]\n",
    "* Univ. of Washington, Allen AI\n",
    "  * Hamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi, \"Segment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing\", ICCV, 2015, [[Paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Izadinia_Segment-Phrase_Table_for_ICCV_2015_paper.pdf)]\n",
    "* INRIA\n",
    "  * Iasonas Kokkinos, \"Pusing the Boundaries of Boundary Detection Using deep Learning\", ICLR 2016, [[Paper](http://arxiv.org/pdf/1511.07386v2.pdf)]\n",
    "* UCSB\n",
    "  * Niloufar Pourian, S. Karthikeyan, and B.S. Manjunath, \"Weakly supervised graph based semantic segmentation by learning communities of image-parts\", ICCV, 2015, [[Paper](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pourian_Weakly_Supervised_Graph_ICCV_2015_paper.pdf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4508f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4120e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Taking an image and identifying the edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84171905",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/edge_detection_example.png)\n",
    "\n",
    "from Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c358f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "\n",
    "[The multi-cue boundary detection dataset](https://serre-lab.clps.brown.edu/resource/multicue/) - We collected two sets of hand-annotations for the last video frame of the left image for every scene: one for object boundaries, and one for “lower-level” edges. Hand-segmentation was performed by paid undergraduate students at Brown University (Providence, RI). We wrote custom custom Java software to enable manual annotations within a web browser. Annotators were not limited in the amount of time they had available to complete the task. The segmentation involved annotating contours that defined the boundary of each object’s visible surface regions. We gave all annotators the same basic instructions as done in Martin, Fowlkes, and Malik (2004): “You will be presented a photographic image. Divide the image into some number of segments, where the segments represent things or parts of things in the scene. The number of segments is up to you, as it depends on the image. Something between 2 and 30 is likely to be appropriate. It is important that all of the segments have approximately equal importance.” \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560362d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Holistically-Nested Edge Detection [[Paper]](http://arxiv.org/pdf/1504.06375) [[Code]](https://github.com/s9xie/hed)\n",
    "  * Saining Xie, Zhuowen Tu, Holistically-Nested Edge Detection, arXiv:1504.06375.\n",
    "* DeepEdge [[Paper]](http://arxiv.org/pdf/1412.1123)\n",
    "  * Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.\n",
    "* DeepContour [[Paper]](http://mc.eistar.net/UpLoadFiles/Papers/DeepContour_cvpr15.pdf)\n",
    "  * Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang, DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection, CVPR, 2015.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07b008",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Colorization\n",
    "Taking an image, or strokes of color and converting it into a colored image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed2b4e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/Colorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556238d0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} More Information\n",
    "\n",
    "[Awesome Image Colorization](https://github.com/MarkMoHR/Awesome-Image-Colorization)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36b148",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Super resolution\n",
    "Taking a low quality image and enhancing its resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b6c8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/Image_superresolution.png)\n",
    "\n",
    "{cite:p}`Dong2016-ad`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be265f9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "\n",
    "[BSD (Berkeley Segmentation Dataset)](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/) - BSD is a dataset used frequently for image denoising and super-resolution. Of the subdatasets, BSD100 is aclassical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. BSD100 is the testing set of the Berkeley segmentation dataset BSD300.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd0116",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image and Video Denoising\n",
    "Techniques to remove noise from images and videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b84390",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/Denoising.jpg)\n",
    "\n",
    "{cite}`Xu2020-ny`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c1d80",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "\n",
    "[The Darmstadt Noise Dataset](https://noise.visinf.tu-darmstadt.de) - Lacking realistic ground truth data, image denoising techniques are traditionally evaluated on images corrupted by synthesized i. i. d. Gaussian noise.  This is quite problematic, since noise in real photographs is not i. i. d. Gaussian and even seemingly minor details of the synthetic noise process, such as whether the noisy values are rounded to integers, can have a significant effect on the relative performance of methods.\n",
    "\n",
    "Hence, we present a novel denoising benchmark, the Darmstadt Noise Dataset (DND). It consists of 50 pairs of real noisy images and corresponding ground truth images that were captured with consumer grade cameras of differing sensor sizes. For each pair, a reference image is taken with the base ISO level while the noisy image is taken with higher ISO and appropriately adjusted exposure time. The reference image undergoes a careful post-processing entailing small camera shift adjustment, linear intensity scaling and removal of low-frequency bias. The post-processed image serves as ground truth for our denoising benchmark.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70d9a74",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} More Information\n",
    "\n",
    "[Awesome Denoise](https://github.com/oneTaken/Awesome-Denoise)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d381e78c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optical flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e1469f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/optical_flows.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9db53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "{cite:p}`Lagemann2021-oq`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70838d47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "[Virtual KITTI](https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/) -Virtual KITTI is a photo-realistic synthetic video dataset designed to learn and evaluate computer vision models for several video understanding tasks: object detection and multi-object tracking, scene-level and instance-level semantic segmentation, optical flow, and depth estimation.\n",
    "\n",
    "Virtual KITTI contains 50 high-resolution monocular videos (21,260 frames) generated from five different virtual worlds in urban settings under different imaging and weather conditions. These worlds were created using the Unity game engine and a novel real-to-virtual cloning method. These photo-realistic synthetic videos are automatically, exactly, and fully annotated for 2D and 3D multi-object tracking and at the pixel level with category, instance, flow, and depth labels\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f5c5b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Human Pose Estimation\n",
    "Predicting the pose of a human in an image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc095569",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/pose_estimation_figure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cbdd36",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "[MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/#) - MPII Human Pose dataset is a state of the art benchmark for evaluation of articulated human pose estimation. The dataset includes around 25K images containing over 40K people with annotated body joints. The images were systematically collected using an established taxonomy of every day human activities. Overall the dataset covers 410 human activities and each image is provided with an activity label. Each image was extracted from a YouTube video and provided with preceding and following un-annotated frames. In addition, for the test set we obtained richer annotations including body part occlusions and 3D torso and head orientations.\n",
    "\n",
    "Following the best practices for the performance evaluation benchmarks in the literature we withhold the test annotations to prevent overfitting and tuning on the test set. We are working on an automatic evaluation server and performance analysis tools based on rich test set annotations.\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745eab69",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation](https://arxiv.org/abs/1911.10529) -[[CODE]](https://github.com/jialee93/Improved-Body-Parts) - Jia Li, Wen Su, Zengfu Wang (AAAI2020)\n",
    "- [An End-to-End Framework for Unsupervised Pose Estimation of Occluded Pedestrians](https://arxiv.org/abs/2002.06429)   - Sudip Das, Perla Sai Raj Kishore, Ujjwal Bhattacharya (Arxiv 2020)\n",
    "- [Transferring Dense Pose to Proximal Animal Classes](https://arxiv.org/abs/2003.00080) - [[CODE]](https://asanakoy.github.io/densepose-evolution/)  - Artsiom Sanakoyeu, Vasil Khalidov, Maureen S. McCarthy, Andrea Vedaldi, Natalia Neverova (CVPR 2020)\n",
    "- [Peeking into occluded joints: A novel framework for crowd pose estimation](https://arxiv.org/abs/2003.10506)   - ingteng Qiu, Xuanye Zhang, Yanran Li, Guanbin Li, Xiaojun Wu, Zixiang Xiong, Xiaoguang Han, Shuguang Cui (Arxiv 2020)\n",
    "- [Motion-supervised Co-Part Segmentation](https://arxiv.org/abs/2004.03234)   - Aliaksandr Siarohin*, Subhankar Roy*, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, Nicu Sebe (Arxiv 2020)\n",
    "- [Detailed 2D-3D Joint Representation for Human-Object Interaction](https://arxiv.org/abs/2004.08154) - [[CODE]](https://github.com/DirtyHarryLYL/DJ-RN)  - Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, Cewu Lu (CVPR 2020)\n",
    "- [Distribution Aware Coordinate Representation for Human Pose Estimation](http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Distribution-Aware_Coordinate_Representation_for_Human_Pose_Estimation_CVPR_2020_paper.pdf) - [[CODE]](https://github.com/ilovepose/DarkPose)  - Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, Ce Zhu (CVPR 2020)\n",
    "- [Yoga-82: A New Dataset for Fine-grained Classification of Human Poses](https://arxiv.org/abs/2004.10362) - [[Data]](https://sites.google.com/view/yoga-82/home)  - Manisha Verma, Sudhakar Kumawat, Yuta Nakashima, Shanmuganathan Raman (CVPRW 2020)\n",
    "- [Self-supervised Keypoint Correspondences for Multi-Person Pose Estimation and Tracking in Videos\n",
    "](https://arxiv.org/abs/2004.12652)   - Rafi Umer, Andreas Doering, Bastian Leibe, Juergen Gall (Arxiv 2020)\n",
    "- [Making DensePose fast and light](https://arxiv.org/abs/2006.15190)   (Arxiv 2020)\n",
    "- [Differentiable Hierarchical Graph Grouping for Multi-Person Pose Estimation](https://arxiv.org/pdf/2007.11864) - Sheng Jin, Wentao Liu, Enze Xie, Wenhai Wang, Chen Qian, Wanli Ouyang, Ping Luo (ECCV 2020)\n",
    "- [Whole-Body Human Pose Estimation in the Wild](https://arxiv.org/abs/2007.11858) - [[Data]](https://github.com/jin-s13/COCO-WholeBody)  - Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo (ECCV 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a14b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6D Object Pose Estimation\n",
    "Models to determine the location and orientation of objects from an image important for robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c83c5d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/centersnap_reconstruction_6d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be567b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "{cite:p}`Irshad2022-hz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146f51b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "[ICCV2015 Occluded Object Challenge](https://hci.iwr.uni-heidelberg.de/vislearn/iccv2015-occlusion-challenge/#Dataset) - The purpose of this challenge is to compare different methods for object pose estimation in a realistic setting featuring heavy occlusion. Our dataset includes eight objects in a cluttered scene. Given a RGB-D image the method has to estimate the position and orientation (a total of six degrees of freedom) of each object. You can participate by applying your method to our data and submitting your results. We will evaluate submitted results according to multiple metrics and display the scores for comparison.\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531180d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.\n",
    "\n",
    "*Source Wikipedia*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313354a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting Knowledge from Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bae2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/Materials_Science_NLP.webp)\n",
    "\n",
    "{cite}`Tshitoyan2019-zt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e60cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "[GLUE (General Language Understanding Evaluation benchmark)](https://gluebenchmark.com/) - General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.\n",
    "\n",
    "[SQuAD (Stanford Question Answering Dataset)](https://rajpurkar.github.io/SQuAD-explorer/) - The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.\n",
    "\n",
    "[SST (Stanford Sentiment Treebank)](https://nlp.stanford.edu/sentiment) - The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.\n",
    "\n",
    "Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive. The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ecf17",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Speech Recognition\n",
    "Learning context representations and information from audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ae1db4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/wav2vec2.png)\n",
    "\n",
    "{cite}`Baevski_undated-gm`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454c7c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```{admonition} Benchmark Datasets\n",
    "[LibriSpeech](http://www.openslr.org/12) - The LibriSpeech corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ’clean’ and ’other’ categories, respectively, depending upon how well or challening Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.\n",
    "\n",
    "[AudioSet](https://research.google.com/audioset/index.html) - Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9edd96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Models\n",
    "Generative models generate new examples from a distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1603ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/generative_v_discriminative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca7d85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In deep learning these models are called Generative Adversarial Networks (GANs) $\\rightarrow$ there are a ton of cool applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88330c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generating Anime Characters\n",
    "\n",
    "![](./figs/Generated_anime.jpg)\n",
    "\n",
    "{cite:p}`Jin2017-gy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee931e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generating Images from Text\n",
    "\n",
    "![](./figs/text_to_image.jpg)\n",
    "\n",
    "{cite}`Dash2017-kw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa4ecb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe\n",
       "    width=\"2200\"\n",
       "    height=\"1200\"\n",
       "    src=\"http://gaugan.org/gaugan2/\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "\n",
       "></iframe>\n"
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1035972b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://gaugan.org/gaugan2/', width=2200, height=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4b6d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Image-to-Image Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae2742",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/cyclegans.jpeg)\n",
    "\n",
    "{cite:p}`Zhu2017-zi`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc62ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DeepFakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c92b39",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Synthetic media where a person is replaced by the likeness (image and voice of another person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79104e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/jupyterbook/lib/python3.10/site-packages/IPython/core/display.py:419: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"602\" height=\"339\" src=\"https://www.youtube.com/embed/cQ54GDm1eL0\" title=\"You Won’t Believe What Obama Says In This Video! 😉\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"602\" height=\"339\" src=\"https://www.youtube.com/embed/cQ54GDm1eL0\" title=\"You Won’t Believe What Obama Says In This Video! 😉\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0158f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./figs/Faceswap.jpeg)\n",
    "\n",
    "from [https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21ebf5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Physics Informed and Physics Constrained Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5e289",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Physics-Informed Neural Networks (PINNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe02d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/PINNS.webp)\n",
    "\n",
    "{cite}`Cai2022-ex`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482baa2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Accelerated Fitting Using Physics-Constrained Neural Networks\n",
    "\n",
    "As long as the empirical functions are differentiable you can train a model to predict the parameters using the empirical function as a decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed25c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Model Architecture\n",
    "\n",
    "![](./figs/AE_image.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678d5f8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Fit Results\n",
    "\n",
    "![](./figs/Fit_results.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbc0c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Learning Underlying Governing Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae1b3d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are ways to take raw data and candidate functions and learn underlying governing equations using sparse identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb060d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./figs/lorenz.jpeg)\n",
    "\n",
    "{cite:p}`Brunton2016-ej`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68194c6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c16269",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Playing Mario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59f82a24",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qv6UVOQ0F44\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qv6UVOQ0F44\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7d20cf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Much More Complex Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a9c01b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UuhECwm31dM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UuhECwm31dM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb882d50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Physical Object Manipulation or Fine Motor Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2739da89",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"1268\" height=\"713\" src=\"https://www.youtube.com/embed/x4O8pojMF0w\" title=\"Solving Rubik’s Cube with a Robot Hand\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"1268\" height=\"713\" src=\"https://www.youtube.com/embed/x4O8pojMF0w\" title=\"Solving Rubik’s Cube with a Robot Hand\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d419f8ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Take Away Messages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0df905",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Data analysis and machine learning can be used in a variety of complex tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea74cc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There are a variety of different implementations and methods for in machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a34a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* We have only just scratched the surface in how machine learning can be applied, this is a very exciting time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec759f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In this class we will prepare you to be a machine learning practitioner, and use machine learning for applications in science and manufacturing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d3c87",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Machine learning is a rapidly growing field. You could take a full course in each of these areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4916cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "rise": {
   "scroll": true
  },
  "vscode": {
   "interpreter": {
    "hash": "5479ed6d567ca5a88c69d3d2da757b3ca4f196df6594dd8ba1c80782f5bfdc5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}