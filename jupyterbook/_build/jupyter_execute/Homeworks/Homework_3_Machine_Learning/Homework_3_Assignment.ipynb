{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Machine Learning Tasks (XX/125 points)\n",
    "## Due Monday 12/7/2022 11:59 pm\n",
    "## About Dataset\n",
    "### Data from a semi-conductor manufacturing process\n",
    "\n",
    "Number of Instances: 1567 <br>\n",
    "Area: Computer<br>\n",
    "Attribute Characteristics: Real<br>\n",
    "Number of Attributes: 591<br>\n",
    "Date Donated: 2008-11-19<br>\n",
    "Associated Tasks: Classification, Causal-Discovery<br>\n",
    "Missing Values? Yes<br>\n",
    "\n",
    "A complex modern semi-conductor manufacturing process is normally under consistent\n",
    "surveillance via the monitoring of signals/variables collected from sensors and or\n",
    "process measurement points. However, not all of these signals are equally valuable\n",
    "in a specific monitoring system. The measured signals contain a combination of\n",
    "useful information, irrelevant information as well as noise. It is often the case\n",
    "that useful information is buried in the latter two. Engineers typically have a\n",
    "much larger number of signals than are actually required. If we consider each type\n",
    "of signal as a feature, then feature selection may be applied to identify the most\n",
    "relevant signals. The Process Engineers may then use these signals to determine key\n",
    "factors contributing to yield excursions downstream in the process. This will\n",
    "enable an increase in process throughput, decreased time to learning and reduce the per unit production costs.\n",
    "\n",
    "To enhance current business improvement techniques the application of feature\n",
    "selection as an intelligent systems technique is being investigated.\n",
    "\n",
    "The dataset presented in this case represents a selection of such features where\n",
    "each example represents a single production entity with associated measured\n",
    "features and the labels represent a simple pass/fail yield for in-house line testing, figure 2, and associated date time stamp. Where -1 corresponds to a pass\n",
    "and 1 corresponds to a fail and the data time stamp is for that specific test\n",
    "point.\n",
    "\n",
    "This homework assignment will walk you through how to tackle this real problem. \n",
    "\n",
    "It is worth noting that this is an actual dataset and thus the problem is not fully tractable. Like many real problems, sometimes you do not have the necessary information to make a perfect solution. We want a useful and informative solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here are some packages and modules that you will use. Make sure they are installed.\n",
    "\n",
    "# for basic operations\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for modeling \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import plotly_express as px\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (5 points)\n",
    "\n",
    "1. The data file is in a file called `uci-secom.csv` scikit-learn works well with pandas. It is recommended that you read the csv into a pandas array.\n",
    "2. It is useful to print the shape of the output array to know what the data is that you are working with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pandas has a built-in method called head that shows the first few rows, this is useful to see what the data looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data (5 points)\n",
    "\n",
    "Real data is usually a mess. There could be missing points, outliers, and features that could have vastly different values and ranges. \n",
    "* Machine learning models are influenced heavily by these problems \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Missing Values (5 Points)\n",
    "\n",
    "1. It is not uncommon that some of the features have only a few entries. These are not helpful for machine learning. We should just remove these features from the data\n",
    "\n",
    "It is good to visualize how many missing values each feature has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Missing Data Entries (5 points)\n",
    "\n",
    "Hint: you can find the nan values with the `.isna()` method, and the sum using `.sum()`\n",
    "\n",
    "You can plot the data using `px.histogram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Sparse Features (10 points)\n",
    "\n",
    "We can remove the features that have more than 100 missing entries. \n",
    "\n",
    "You can find the location where a condition is met in a Pandas array using `data.loc[:,:]` with traditional numpy-like indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove these columns in the dataframe using the `.drop()` method, make sure inplace is set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to check the shape to make sure that the operation worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to see how many data points have missing information\n",
    "\n",
    "Hint: you can change the axis of the `isna()`\n",
    "\n",
    "You can use the built-in method `value_counts()` to view the number of samples with bad rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is not that many we can remove these rows.\n",
    "* you can just index the data that you want using the built-in method `.loc()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should grab the features and labels you can do this by:\n",
    "1. Using the Pandas `drop` built-in method, you can also drop the time\n",
    "2. You should set the the prediction to be if the dataset passed or failed\n",
    "3. It is a good idea to replace the -1 values with 0 using the pandas built-in method `.replace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-train split (5 points)\n",
    "\n",
    "Use the `train_test_split` method to split the data.\n",
    "\n",
    "For consistency make the `test_size = .3`, and the `random_state = 42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning (5 points)\n",
    "\n",
    "It is always good to try a quick machine learning model. If your data is simple it might just work.\n",
    "\n",
    "* Implement a `LogisticRegression` from scikit-learn\n",
    "* Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to see if the model fit the data well.\n",
    "* uses the `.predict()` method to predict on the training data\n",
    "* use the sklearn function `classification_report` to evaluate the model\n",
    "* use the sklearn function `confusion_matrix`, you can plot this in plotly using `px.imshow()`\n",
    "\n",
    "You will reuse these lines of code to visualize your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the same approach to visualize the test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Question: Describe what might be wrong with this model, does it provide any practical value? (5 points)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Another Model (5 points)\n",
    "\n",
    "It could be that we just selected a bad model for the problem, try with a random forest classifier as implemented in scikit-learn\n",
    "* Instantiate the model\n",
    "* Fit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the model on the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That still does not do anything meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data (10 points)\n",
    "\n",
    "Machine learning models prefer features with a mean of 0 and a standard deviation of 1. This makes the optimization easier.\n",
    "\n",
    "1. Make a histogram of the mean and standard deviation you can use the built-in method `.mean()` and `.std()`\n",
    "2. You can plot this using `px.histogram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a utility function for conducting standard scalars `StandardScaler()`\n",
    "\n",
    "We could implement the standard scaler in steps but it is more convenient to do it with a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Logistic Regression (5 points)\n",
    "\n",
    "1. Use the `Pipeline` utility to create a machine learning model that:\n",
    "    - Computes the standard scalar of the data\n",
    "    - Conducts logistic regression\n",
    "2. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results as you have done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaled Random Forest (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the `Pipeline` utility to create a machine learning model that:\n",
    "    - Computes the standard scalar of the data\n",
    "    - Conducts Random Forest\n",
    "2. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Question: Explain what is going on with the random forest model? Why are the results so bad? (5 points)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Reduction\n",
    "\n",
    "### Logistic Regression (5 Points)\n",
    "\n",
    "We can use PCA to reduce the number of features such that highly covariant features are combined. This helps deal with the curse of dimensionality.\n",
    "\n",
    "Add PCA to the pipeline for the logistic regression, and visualize the results as we have done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here to build and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here to validate the training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here to validate the test performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (5 points)\n",
    "\n",
    "Add PCA to the pipeline for the logistic regression, and visualize the results as we have done before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here to build and fit the feature reduced random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here to visualize the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here to visualize the test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Question: Explain if adding PCA helped, explain why you think PCA helped or did not help. (5 points)</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning \n",
    "\n",
    "To improve a machine learning model you might want to tune the hyperparameters.\n",
    "\n",
    "Scikit-learn has automated tools for cross-validation and hyperparameter search. You can just define a dictionary of values that you want to search and it will try all of the fits returning the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (7.5 points)\n",
    "\n",
    "Conduct build a pipeline and build a parameter grid to search the following hyperparameters:\n",
    "1. C = [ 0.001, .01, .1, 1, 10, 100]\n",
    "2. penalty = ['l1', 'l2']\n",
    "3. class_weight = ['balanced']\n",
    "4. solver = ['saga']\n",
    "5. PCA n_components = [2, 3, 4, 5, 8, 10]\n",
    "\n",
    "To conduct the fitting you should build the classifier with GridSearchCV. This conducts a grid search with cross-folds. See the documentation for more information. \n",
    "\n",
    "For the GridSearchCV set the scoring to 'f1', cv=5. If you want to monitor the status you can set verbose=10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should look and see what the best estimator was from the search. \n",
    "\n",
    "You can use the `best_estimator_` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to evaluate the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here for validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (7.5 points)\n",
    "\n",
    "Conduct build a pipeline and build a parameter grid to search the following hyperparameters:\n",
    "1. Random Forest Criterion = [ \"gini\", \"entropy\", \"log_loss\"]\n",
    "2. max depth = [4, 8, 12]\n",
    "3. max features = ['sqrt', 'log2']\n",
    "4. PCA n_components = [4, 8, 10, 20]\n",
    "\n",
    "To conduct the fitting you should build the classifier with GridSearchCV. This conducts a grid search with cross-folds. See the documentation for more information. \n",
    "\n",
    "For the GridSearchCV set the scoring to 'f1', cv=5. If you want to monitor the status you can set verbose=10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to show the best estimator goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to evaluate the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing in the Data (10 points)\n",
    "\n",
    "This is a classification problem, it is useful to see if the classes are balanced as this affects model training. \n",
    "\n",
    "If you have a highly unbalanced dataset you can train a model to predict the most common classes but getting the uncommon classes wrong has little effect on the model performance metrics.\n",
    "\n",
    "View the ratio of the class outcomes. \n",
    "\n",
    "The class outcomes are stored in the ['pass/fail'] column, you can view the values and counts using the `.value_count()` built-in method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `SMOTE(`)` to balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that this worked as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the balanced dataset repeat the analysis done with the hyperparameter search\n",
    "\n",
    "### Logistic Regression (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your fitting code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to show the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to check the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to validate the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to conduct the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to show the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to validate the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code to validate the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Question: Given that you are trying to predict and determine the underlying features responsible for producing products that \"pass\" which model would be better and why? (5 points)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (10 Points): \n",
    "\n",
    "Use any method available to you to get a better validation F1 score for the Pass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trick to always get better results is to use ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('jupyterbook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c475b5beda6d617ffb7b2fcf453fbe132321ffc1e1f96c06cf49356e1e7f42cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}