
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>What You Can Do With Machine Learning &#8212; MEM T680: Fall 2022: Data Analysis and Machine Learning</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/Drexel_M3_Logo-01.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/Drexel_M3_Logo-01.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">MEM T680: Fall 2022: Data Analysis and Machine Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    MEM T680: Data Analysis and Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Course Content üìú
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Course_Materials/Course_materials.html">
   Course Materials
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Course_Materials/syllabus.html">
     Syllabus
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Course_Materials/Course_survey.html">
     Course Survey
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Course_Materials/Survey_Response.html">
     Survey Responses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Course_Materials/Final_project.html">
     Final Project
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 1Ô∏è‚É£
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Topic_1/Objectives.html">
   Importance and Challenges in Data Analysis
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Topic_1/T1_Importance_of_Data_Analysis/Importance_of_Data_Analysis.html">
     Importance of Data Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Topic_1/T2_Challenges_in_Data_Analysis/Challenges%20in%20Data%20Analysis.html">
     Challenges in Data Analysis
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Topic 2Ô∏è‚É£
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Topic_2/Objectives.html">
   Setting Up Your Computing Environment
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Topic_2/1_Introduction_to_MEMT680/Introduction_to_MEMT680.html">
     Introduction to the Course Machinery
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Topic_2/2_Navigating_Jupyter/navigating_jupyter.html">
     Using Project Jupyter
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Topic_2/3_setting_up_your_computing_env/setting_up_your_computing_environment.html">
     Setting up your Computing Environment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Topic_2/4_working_in_colab/Welcome_To_Colaboratory.html">
     Welcome to Colaboratory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Topic_2/5_Introduction_to_git/Introduction_to_GIT.html">
     Introduction to GitHub
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  üìù Homework
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Homeworks/Homeworks.html">
   Homework Assignments
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Homeworks/Homework_1_Designing%20A%20Metadata%20Schema/Homework%201%20Designing%20a%20Metadata%20Schema.html">
     üìù Homework 1 Designing a Metadata Schema
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/jagar2/Fall_2022_MEM_T680Data_Analysis_and_Machine_Learning/main?urlpath=tree/jupyterbook/What_you_can_do_with_machine_learning/What_you_can_do_with_machine_learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://jupyterhub.coe.drexel.edu/hub/user-redirect/git-pull?repo=https%3A//github.com/jagar2/Fall_2022_MEM_T680Data_Analysis_and_Machine_Learning/tree/main/jupyterbook&urlpath=tree/Fall_2022_MEM_T680Data_Analysis_and_Machine_Learning/jupyterbook/What_you_can_do_with_machine_learning/What_you_can_do_with_machine_learning.ipynb&branch=main"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on JupyterHub"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_jupyterhub.svg">
  </span>
<span class="headerbtn__text-container">JupyterHub</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/jagar2/Fall_2022_MEM_T680Data_Analysis_and_Machine_Learning/blob/main/jupyterbook/What_you_can_do_with_machine_learning/What_you_can_do_with_machine_learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/jagar2/Fall_2022_MEM_T680Data_Analysis_and_Machine_Learning/tree/main/jupyterbook"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/jagar2/Fall_2022_MEM_T680Data_Analysis_and_Machine_Learning/tree/main/jupyterbook/issues/new?title=Issue%20on%20page%20%2FWhat_you_can_do_with_machine_learning/What_you_can_do_with_machine_learning.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/What_you_can_do_with_machine_learning/What_you_can_do_with_machine_learning.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computer-vision-and-graphics">
   Computer Vision and Graphics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-classifiers">
     Image Classifiers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-model-benchmarks">
     Classification Model Benchmarks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#object-detection">
     Object Detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#semantic-segmentation">
     Semantic Segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edge-detection">
     Edge Detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#colorization">
     Colorization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#super-resolution">
     Super resolution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-and-video-denoising">
     Image and Video Denoising
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optical-flows">
     Optical flows
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#human-pose-estimation">
     Human Pose Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-object-pose-estimation">
     6D Object Pose Estimation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#natural-language-processing">
   Natural Language Processing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extracting-knowledge-from-text-corpus">
     Extracting Knowledge from Text Corpus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#speech-recognition">
     Speech Recognition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-models">
   Generative Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-anime-characters">
     Generating Anime Characters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-images-from-text">
     Generating Images from Text
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-to-image-translation">
     Image-to-Image Translation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deepfakes">
     DeepFakes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#physics-informed-and-physics-constrained-machine-learning">
   Physics Informed and Physics Constrained Machine Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#physics-informed-neural-networks-pinns">
     Physics-Informed Neural Networks (PINNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accelerated-fitting-using-physics-constrained-neural-networks">
     Accelerated Fitting Using Physics-Constrained Neural Networks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-architecture">
       Model Architecture
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-results">
       Fit Results
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-underlying-governing-equations">
     Learning Underlying Governing Equations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reinforcement-learning">
   Reinforcement Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#playing-mario">
     Playing Mario
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#much-more-complex-games">
     Much More Complex Games
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#physical-object-manipulation-or-fine-motor-skills">
     Physical Object Manipulation or Fine Motor Skills
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#take-away-messages">
   Take Away Messages:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>What You Can Do With Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computer-vision-and-graphics">
   Computer Vision and Graphics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-classifiers">
     Image Classifiers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification-model-benchmarks">
     Classification Model Benchmarks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#object-detection">
     Object Detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#semantic-segmentation">
     Semantic Segmentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edge-detection">
     Edge Detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#colorization">
     Colorization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#super-resolution">
     Super resolution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-and-video-denoising">
     Image and Video Denoising
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optical-flows">
     Optical flows
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#human-pose-estimation">
     Human Pose Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#d-object-pose-estimation">
     6D Object Pose Estimation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#natural-language-processing">
   Natural Language Processing
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#extracting-knowledge-from-text-corpus">
     Extracting Knowledge from Text Corpus
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#speech-recognition">
     Speech Recognition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generative-models">
   Generative Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-anime-characters">
     Generating Anime Characters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-images-from-text">
     Generating Images from Text
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-to-image-translation">
     Image-to-Image Translation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deepfakes">
     DeepFakes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#physics-informed-and-physics-constrained-machine-learning">
   Physics Informed and Physics Constrained Machine Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#physics-informed-neural-networks-pinns">
     Physics-Informed Neural Networks (PINNs)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#accelerated-fitting-using-physics-constrained-neural-networks">
     Accelerated Fitting Using Physics-Constrained Neural Networks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-architecture">
       Model Architecture
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#fit-results">
       Fit Results
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#learning-underlying-governing-equations">
     Learning Underlying Governing Equations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reinforcement-learning">
   Reinforcement Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#playing-mario">
     Playing Mario
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#much-more-complex-games">
     Much More Complex Games
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#physical-object-manipulation-or-fine-motor-skills">
     Physical Object Manipulation or Fine Motor Skills
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#take-away-messages">
   Take Away Messages:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="what-you-can-do-with-machine-learning">
<h1>What You Can Do With Machine Learning<a class="headerlink" href="#what-you-can-do-with-machine-learning" title="Permalink to this headline">#</a></h1>
<section id="computer-vision-and-graphics">
<h2>Computer Vision and Graphics<a class="headerlink" href="#computer-vision-and-graphics" title="Permalink to this headline">#</a></h2>
<section id="image-classifiers">
<h3>Image Classifiers<a class="headerlink" href="#image-classifiers" title="Permalink to this headline">#</a></h3>
<p>The most common machine learning task is to take an image and classify if a single object is in an image</p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> - The MNIST database of handwritten digits.</p>
<p><a class="reference external" href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">CAL 101</a> - Caltech-101 consists of pictures of objects belonging to 101 classes, plus one background clutter class. Each image is labelled with a single object. Each class contains roughly 40 to 800 images, totalling around 9k images. Images are of variable sizes, with typical edge lengths of 200-300 pixels. This version contains image-level labels only. The original dataset also contains bounding boxes.</p>
<p><a class="reference external" href="https://www.image-net.org">ImageNet</a> -ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. The project has been instrumental in advancing computer vision and deep learning research. The data is available for free to researchers for non-commercial use.</p>
<p><a class="reference external" href="https://www.cs.toronto.edu/~kriz/cifar.html">CFAR 10</a> - The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.</p>
</div>
<p><img alt="" src="../_images/Classification.png" /></p>
<p>from Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NeuroIPS, 2012.</p>
<ul class="simple">
<li><p>Microsoft (Deep Residual Learning) [<a class="reference external" href="http://arxiv.org/pdf/1512.03385v1.pdf">Paper</a>][<a class="reference external" href="http://image-net.org/challenges/talks/ilsvrc2015_deep_residual_learning_kaiminghe.pdf">Slide</a>]</p>
<ul>
<li><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition, arXiv:1512.03385.</p></li>
</ul>
</li>
<li><p>Microsoft (PReLu/Weight Initialization) <a class="reference external" href="http://arxiv.org/pdf/1502.01852">[Paper]</a></p>
<ul>
<li><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, arXiv:1502.01852.</p></li>
</ul>
</li>
<li><p>Batch Normalization <a class="reference external" href="http://arxiv.org/pdf/1502.03167">[Paper]</a></p>
<ul>
<li><p>Sergey Ioffe, Christian Szegedy, Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, arXiv:1502.03167.</p></li>
</ul>
</li>
<li><p>GoogLeNet <a class="reference external" href="http://arxiv.org/pdf/1409.4842">[Paper]</a></p>
<ul>
<li><p>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR, 2015.</p></li>
</ul>
</li>
<li><p>VGG-Net <a class="reference external" href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">[Web]</a> <a class="reference external" href="http://arxiv.org/pdf/1409.1556">[Paper]</a></p>
<ul>
<li><p>Karen Simonyan and Andrew Zisserman, Very Deep Convolutional Networks for Large-Scale Visual Recognition, ICLR, 2015.</p></li>
</ul>
</li>
<li><p>AlexNet <a class="reference external" href="http://papers.nips.cc/book/advances-in-neural-information-processing-systems-25-2012">[Paper]</a></p>
<ul>
<li><p>Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, NIPS, 2012.</p></li>
</ul>
</li>
</ul>
</section>
<section id="classification-model-benchmarks">
<h3>Classification Model Benchmarks<a class="headerlink" href="#classification-model-benchmarks" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/imagenet_benchmarks.jpg" />
<a class="reference external" href="https://paperswithcode.com/sota/image-classification-on-imagenet">ImageNet Leaderboard</a></p>
</section>
<section id="object-detection">
<h3>Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this headline">#</a></h3>
<p>Models that try to find and detect the location of multiple objects in an image</p>
<p><img alt="" src="../_images/object_detection.png" /></p>
<p>from Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497</p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="http://cocodataset.org/#home">COCO</a> - COCO is a large-scale object detection, segmentation, and captioning dataset.</p>
<p><a class="reference external" href="https://storage.googleapis.com/openimages/web/index.html">open_images_v4</a> - Open Images is a dataset of ~9M images that have been annotated with image-level labels and object bounding boxes.</p>
<p>The training set of V4 contains 14.6M bounding boxes for 600 object classes on 1.74M images, making it the largest existing dataset with object location annotations. The boxes have been largely manually drawn by professional annotators to ensure accuracy and consistency. The images are very diverse and often contain complex scenes with several objects (8.4 per image on average). Moreover, the dataset is annotated with image-level labels spanning thousands of classes.</p>
</div>
<ul class="simple">
<li><p>PVANET <a class="reference external" href="https://arxiv.org/pdf/1608.08021">[Paper]</a> <a class="reference external" href="https://github.com/sanghoon/pva-faster-rcnn">[Code]</a></p>
<ul>
<li><p>Kye-Hyeon Kim, Sanghoon Hong, Byungseok Roh, Yeongjae Cheon, Minje Park, PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection, arXiv:1608.08021</p></li>
</ul>
</li>
<li><p>OverFeat, NYU <a class="reference external" href="http://arxiv.org/pdf/1312.6229.pdf">[Paper]</a></p>
<ul>
<li><p>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks, ICLR, 2014.</p></li>
</ul>
</li>
<li><p>R-CNN, UC Berkeley <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">[Paper-CVPR14]</a> <a class="reference external" href="http://arxiv.org/pdf/1311.2524">[Paper-arXiv14]</a></p>
<ul>
<li><p>Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, Rich feature hierarchies for accurate object detection and semantic segmentation, CVPR, 2014.</p></li>
</ul>
</li>
<li><p>SPP, Microsoft Research <a class="reference external" href="http://arxiv.org/pdf/1406.4729">[Paper]</a></p>
<ul>
<li><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, ECCV, 2014.</p></li>
</ul>
</li>
<li><p>Fast R-CNN, Microsoft Research <a class="reference external" href="http://arxiv.org/pdf/1504.08083">[Paper]</a></p>
<ul>
<li><p>Ross Girshick, Fast R-CNN, arXiv:1504.08083.</p></li>
</ul>
</li>
<li><p>Faster R-CNN, Microsoft Research <a class="reference external" href="http://arxiv.org/pdf/1506.01497">[Paper]</a></p>
<ul>
<li><p>Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun, Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, arXiv:1506.01497.</p></li>
</ul>
</li>
<li><p>R-CNN minus R, Oxford <a class="reference external" href="http://arxiv.org/pdf/1506.06981">[Paper]</a></p>
<ul>
<li><p>Karel Lenc, Andrea Vedaldi, R-CNN minus R, arXiv:1506.06981.</p></li>
</ul>
</li>
<li><p>End-to-end people detection in crowded scenes <a class="reference external" href="http://arxiv.org/abs/1506.04878">[Paper]</a></p>
<ul>
<li><p>Russell Stewart, Mykhaylo Andriluka, End-to-end people detection in crowded scenes, arXiv:1506.04878.</p></li>
</ul>
</li>
<li><p>You Only Look Once: Unified, Real-Time Object Detection <a class="reference external" href="http://arxiv.org/abs/1506.02640">[Paper]</a>, <a class="reference external" href="https://arxiv.org/abs/1612.08242">[Paper Version 2]</a>, <a class="reference external" href="https://github.com/pjreddie/darknet">[C Code]</a>, <a class="reference external" href="https://github.com/thtrieu/darkflow">[Tensorflow Code]</a></p>
<ul>
<li><p>Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi, You Only Look Once: Unified, Real-Time Object Detection, arXiv:1506.02640</p></li>
<li><p>Joseph Redmon, Ali Farhadi (Version 2)</p></li>
</ul>
</li>
<li><p>Inside-Outside Net <a class="reference external" href="http://arxiv.org/abs/1512.04143">[Paper]</a></p>
<ul>
<li><p>Sean Bell, C. Lawrence Zitnick, Kavita Bala, Ross Girshick, Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</p></li>
</ul>
</li>
<li><p>Deep Residual Network (Current State-of-the-Art) <a class="reference external" href="http://arxiv.org/abs/1512.03385">[Paper]</a></p>
<ul>
<li><p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition</p></li>
</ul>
</li>
<li><p>Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning [<a class="reference external" href="http://arxiv.org/pdf/1503.00949.pdf">Paper</a>]</p></li>
<li><p>R-FCN <a class="reference external" href="https://arxiv.org/abs/1605.06409">[Paper]</a> <a class="reference external" href="https://github.com/daijifeng001/R-FCN">[Code]</a></p>
<ul>
<li><p>Jifeng Dai, Yi Li, Kaiming He, Jian Sun, R-FCN: Object Detection via Region-based Fully Convolutional Networks</p></li>
</ul>
</li>
<li><p>SSD <a class="reference external" href="https://arxiv.org/pdf/1512.02325v2.pdf">[Paper]</a> <a class="reference external" href="https://github.com/weiliu89/caffe/tree/ssd">[Code]</a></p>
<ul>
<li><p>Wei Liu1, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, Alexander C. Berg, SSD: Single Shot MultiBox Detector, arXiv:1512.02325</p></li>
</ul>
</li>
<li><p>Speed/accuracy trade-offs for modern convolutional object detectors <a class="reference external" href="https://arxiv.org/pdf/1611.10012v1.pdf">[Paper]</a></p>
<ul>
<li><p>Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, Kevin Murphy, Google Research, arXiv:1611.10012</p></li>
</ul>
</li>
</ul>
</section>
<section id="semantic-segmentation">
<h3>Semantic Segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this headline">#</a></h3>
<p>The task of clustering parts of an image together which belong to the same object class. It is a form of pixel-level prediction because each pixel in an image is classified according to a category</p>
<p><img alt="" src="../_images/semantic_segmentation.png" /></p>
<p>from Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640.</p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL VOC (PASCAL Visual Object Classes Challenge)</a> - The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories including vehicles, household, animals, and other: aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse, sheep, and person. Each image in this dataset has pixel-level segmentation annotations, bounding box annotations, and object class annotations. This dataset has been widely used as a benchmark for object detection, semantic segmentation, and classification tasks. The PASCAL VOC dataset is split into three subsets: 1,464 images for training, 1,449 images for validation and a private testing set.</p>
</div>
<ul class="simple">
<li><p>SEC: Seed, Expand and Constrain</p>
<ul>
<li><p>Alexander Kolesnikov, Christoph Lampert, Seed, Expand and Constrain: Three Principles for Weakly-Supervised Image Segmentation, ECCV, 2016. <a class="reference external" href="http://pub.ist.ac.at/~akolesnikov/files/ECCV2016/main.pdf">[Paper]</a> <a class="reference external" href="https://github.com/kolesman/SEC">[Code]</a></p></li>
</ul>
</li>
<li><p>Adelaide</p>
<ul>
<li><p>Guosheng Lin, Chunhua Shen, Ian Reid, Anton van dan Hengel, Efficient piecewise training of deep structured models for semantic segmentation, arXiv:1504.01013. <a class="reference external" href="http://arxiv.org/pdf/1504.01013">[Paper]</a> (1st ranked in VOC2012)</p></li>
<li><p>Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel, Deeply Learning the Messages in Message Passing Inference, arXiv:1508.02108. <a class="reference external" href="http://arxiv.org/pdf/1506.02108">[Paper]</a> (4th ranked in VOC2012)</p></li>
</ul>
</li>
<li><p>Deep Parsing Network (DPN)</p>
<ul>
<li><p>Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, Xiaoou Tang, Semantic Image Segmentation via Deep Parsing Network, arXiv:1509.02634 / ICCV 2015 <a class="reference external" href="http://arxiv.org/pdf/1509.02634.pdf">[Paper]</a> (2nd ranked in VOC 2012)</p></li>
</ul>
</li>
<li><p>CentraleSuperBoundaries, INRIA <a class="reference external" href="http://arxiv.org/pdf/1511.07386">[Paper]</a></p>
<ul>
<li><p>Iasonas Kokkinos, Surpassing Humans in Boundary Detection using Deep Learning, arXiv:1411.07386 (4th ranked in VOC 2012)</p></li>
</ul>
</li>
<li><p>BoxSup <a class="reference external" href="http://arxiv.org/pdf/1503.01640">[Paper]</a></p>
<ul>
<li><p>Jifeng Dai, Kaiming He, Jian Sun, BoxSup: Exploiting Bounding Boxes to Supervise Convolutional Networks for Semantic Segmentation, arXiv:1503.01640. (6th ranked in VOC2012)</p></li>
</ul>
</li>
<li><p>POSTECH</p>
<ul>
<li><p>Hyeonwoo Noh, Seunghoon Hong, Bohyung Han, Learning Deconvolution Network for Semantic Segmentation, arXiv:1505.04366. <a class="reference external" href="http://arxiv.org/pdf/1505.04366">[Paper]</a> (7th ranked in VOC2012)</p></li>
<li><p>Seunghoon Hong, Hyeonwoo Noh, Bohyung Han, Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation, arXiv:1506.04924. <a class="reference external" href="http://arxiv.org/pdf/1506.04924">[Paper]</a></p></li>
<li><p>Seunghoon Hong,Junhyuk Oh,	Bohyung Han, and	Honglak Lee, Learning Transferrable Knowledge for Semantic Segmentation with Deep Convolutional Neural Network, arXiv:1512.07928 [<a class="reference external" href="http://arxiv.org/pdf/1512.07928.pdf">Paper</a>] [<a class="reference external" href="http://cvlab.postech.ac.kr/research/transfernet/">Project Page</a>]</p></li>
</ul>
</li>
<li><p>Conditional Random Fields as Recurrent Neural Networks <a class="reference external" href="http://arxiv.org/pdf/1502.03240">[Paper]</a></p>
<ul>
<li><p>Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang Huang, Philip H. S. Torr, Conditional Random Fields as Recurrent Neural Networks, arXiv:1502.03240. (8th ranked in VOC2012)</p></li>
</ul>
</li>
<li><p>DeepLab</p>
<ul>
<li><p>Liang-Chieh Chen, George Papandreou, Kevin Murphy, Alan L. Yuille, Weakly-and semi-supervised learning of a DCNN for semantic image segmentation, arXiv:1502.02734. <a class="reference external" href="http://arxiv.org/pdf/1502.02734">[Paper]</a> (9th ranked in VOC2012)</p></li>
</ul>
</li>
<li><p>Zoom-out <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Mostajabi_Feedforward_Semantic_Segmentation_2015_CVPR_paper.pdf">[Paper]</a></p>
<ul>
<li><p>Mohammadreza Mostajabi, Payman Yadollahpour, Gregory Shakhnarovich, Feedforward Semantic Segmentation With Zoom-Out Features, CVPR, 2015</p></li>
</ul>
</li>
<li><p>Joint Calibration <a class="reference external" href="http://arxiv.org/pdf/1507.01581">[Paper]</a></p>
<ul>
<li><p>Holger Caesar, Jasper Uijlings, Vittorio Ferrari, Joint Calibration for Semantic Segmentation, arXiv:1507.01581.</p></li>
</ul>
</li>
<li><p>Fully Convolutional Networks for Semantic Segmentation <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">[Paper-CVPR15]</a> <a class="reference external" href="http://arxiv.org/pdf/1411.4038">[Paper-arXiv15]</a></p>
<ul>
<li><p>Jonathan Long, Evan Shelhamer, Trevor Darrell, Fully Convolutional Networks for Semantic Segmentation, CVPR, 2015.</p></li>
</ul>
</li>
<li><p>Hypercolumn <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Hariharan_Hypercolumns_for_Object_2015_CVPR_paper.pdf">[Paper]</a></p>
<ul>
<li><p>Bharath Hariharan, Pablo Arbelaez, Ross Girshick, Jitendra Malik, Hypercolumns for Object Segmentation and Fine-Grained Localization, CVPR, 2015.</p></li>
</ul>
</li>
<li><p>Deep Hierarchical Parsing</p>
<ul>
<li><p>Abhishek Sharma, Oncel Tuzel, David W. Jacobs, Deep Hierarchical Parsing for Semantic Segmentation, CVPR, 2015. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Sharma_Deep_Hierarchical_Parsing_2015_CVPR_paper.pdf">[Paper]</a></p></li>
</ul>
</li>
<li><p>Learning Hierarchical Features for Scene Labeling <a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/farabet-icml-12.pdf">[Paper-ICML12]</a> <a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf">[Paper-PAMI13]</a></p>
<ul>
<li><p>Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers, ICML, 2012.</p></li>
<li><p>Clement Farabet, Camille Couprie, Laurent Najman, Yann LeCun, Learning Hierarchical Features for Scene Labeling, PAMI, 2013.</p></li>
</ul>
</li>
<li><p>University of Cambridge <a class="reference external" href="http://mi.eng.cam.ac.uk/projects/segnet/">[Web]</a></p>
<ul>
<li><p>Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla ‚ÄúSegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation.‚Äù arXiv preprint arXiv:1511.00561, 2015. <a class="reference external" href="http://arxiv.org/abs/1511.00561">[Paper]</a></p></li>
</ul>
</li>
<li><p>Alex Kendall, Vijay Badrinarayanan and Roberto Cipolla ‚ÄúBayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder Architectures for Scene Understanding.‚Äù arXiv preprint arXiv:1511.02680, 2015. <a class="reference external" href="http://arxiv.org/abs/1511.00561">[Paper]</a></p></li>
<li><p>Princeton</p>
<ul>
<li><p>Fisher Yu, Vladlen Koltun, ‚ÄúMulti-Scale Context Aggregation by Dilated Convolutions‚Äù, ICLR 2016, [<a class="reference external" href="http://arxiv.org/pdf/1511.07122v2.pdf">Paper</a>]</p></li>
</ul>
</li>
<li><p>Univ. of Washington, Allen AI</p>
<ul>
<li><p>Hamid Izadinia, Fereshteh Sadeghi, Santosh Kumar Divvala, Yejin Choi, Ali Farhadi, ‚ÄúSegment-Phrase Table for Semantic Segmentation, Visual Entailment and Paraphrasing‚Äù, ICCV, 2015, [<a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Izadinia_Segment-Phrase_Table_for_ICCV_2015_paper.pdf">Paper</a>]</p></li>
</ul>
</li>
<li><p>INRIA</p>
<ul>
<li><p>Iasonas Kokkinos, ‚ÄúPusing the Boundaries of Boundary Detection Using deep Learning‚Äù, ICLR 2016, [<a class="reference external" href="http://arxiv.org/pdf/1511.07386v2.pdf">Paper</a>]</p></li>
</ul>
</li>
<li><p>UCSB</p>
<ul>
<li><p>Niloufar Pourian, S. Karthikeyan, and B.S. Manjunath, ‚ÄúWeakly supervised graph based semantic segmentation by learning communities of image-parts‚Äù, ICCV, 2015, [<a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Pourian_Weakly_Supervised_Graph_ICCV_2015_paper.pdf">Paper</a>]</p></li>
</ul>
</li>
</ul>
</section>
<section id="edge-detection">
<h3>Edge Detection<a class="headerlink" href="#edge-detection" title="Permalink to this headline">#</a></h3>
<p>Taking an image and identifying the edges</p>
<p><img alt="" src="../_images/edge_detection_example.png" /></p>
<p>from Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.</p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="https://serre-lab.clps.brown.edu/resource/multicue/">The multi-cue boundary detection dataset</a> - We collected two sets of hand-annotations for the last video frame of the left image for every scene: one for object boundaries, and one for ‚Äúlower-level‚Äù edges. Hand-segmentation was performed by paid undergraduate students at Brown University (Providence, RI). We wrote custom custom Java software to enable manual annotations within a web browser. Annotators were not limited in the amount of time they had available to complete the task. The segmentation involved annotating contours that defined the boundary of each object‚Äôs visible surface regions. We gave all annotators the same basic instructions as done in Martin, Fowlkes, and Malik (2004): ‚ÄúYou will be presented a photographic image. Divide the image into some number of segments, where the segments represent things or parts of things in the scene. The number of segments is up to you, as it depends on the image. Something between 2 and 30 is likely to be appropriate. It is important that all of the segments have approximately equal importance.‚Äù</p>
</div>
<ul class="simple">
<li><p>Holistically-Nested Edge Detection <a class="reference external" href="http://arxiv.org/pdf/1504.06375">[Paper]</a> <a class="reference external" href="https://github.com/s9xie/hed">[Code]</a></p>
<ul>
<li><p>Saining Xie, Zhuowen Tu, Holistically-Nested Edge Detection, arXiv:1504.06375.</p></li>
</ul>
</li>
<li><p>DeepEdge <a class="reference external" href="http://arxiv.org/pdf/1412.1123">[Paper]</a></p>
<ul>
<li><p>Gedas Bertasius, Jianbo Shi, Lorenzo Torresani, DeepEdge: A Multi-Scale Bifurcated Deep Network for Top-Down Contour Detection, CVPR, 2015.</p></li>
</ul>
</li>
<li><p>DeepContour <a class="reference external" href="http://mc.eistar.net/UpLoadFiles/Papers/DeepContour_cvpr15.pdf">[Paper]</a></p>
<ul>
<li><p>Wei Shen, Xinggang Wang, Yan Wang, Xiang Bai, Zhijiang Zhang, DeepContour: A Deep Convolutional Feature Learned by Positive-Sharing Loss for Contour Detection, CVPR, 2015.</p></li>
</ul>
</li>
</ul>
</section>
<section id="colorization">
<h3>Colorization<a class="headerlink" href="#colorization" title="Permalink to this headline">#</a></h3>
<p>Taking an image, or strokes of color and converting it into a colored image</p>
<p><img alt="" src="../_images/Colorization.png" /></p>
<div class="admonition-more-information admonition">
<p class="admonition-title">More Information</p>
<p><a class="reference external" href="https://github.com/MarkMoHR/Awesome-Image-Colorization">Awesome Image Colorization</a></p>
</div>
</section>
<section id="super-resolution">
<h3>Super resolution<a class="headerlink" href="#super-resolution" title="Permalink to this headline">#</a></h3>
<p>Taking a low quality image and enhancing its resolution</p>
<p><img alt="" src="../_images/Image_superresolution.png" /></p>
<p><span id="id1">[<a class="reference internal" href="#id20" title="Chao Dong, Chen Change Loy, and Xiaoou Tang. Accelerating the Super-Resolution convolutional neural network. Computer Vision and Pattern Recognition, August 2016. arXiv:1608.00367.">DLT16</a>]</span></p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/">BSD (Berkeley Segmentation Dataset)</a> - BSD is a dataset used frequently for image denoising and super-resolution. Of the subdatasets, BSD100 is aclassical image dataset having 100 test images proposed by Martin et al.. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. BSD100 is the testing set of the Berkeley segmentation dataset BSD300.</p>
</div>
</section>
<section id="image-and-video-denoising">
<h3>Image and Video Denoising<a class="headerlink" href="#image-and-video-denoising" title="Permalink to this headline">#</a></h3>
<p>Techniques to remove noise from images and videos</p>
<p><img alt="" src="../_images/Denoising.jpg" /></p>
<p><span id="id2">[<a class="reference internal" href="#id30" title="Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, and Ling Shao. Noisy-As-Clean: learning self-supervised denoising from corrupted image. IEEE Trans. Image Process., September 2020.">XHC+20</a>]</span></p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="https://noise.visinf.tu-darmstadt.de">The Darmstadt Noise Dataset</a> - Lacking realistic ground truth data, image denoising techniques are traditionally evaluated on images corrupted by synthesized i. i. d. Gaussian noise.  This is quite problematic, since noise in real photographs is not i. i. d. Gaussian and even seemingly minor details of the synthetic noise process, such as whether the noisy values are rounded to integers, can have a significant effect on the relative performance of methods.</p>
<p>Hence, we present a novel denoising benchmark, the Darmstadt Noise Dataset (DND). It consists of 50 pairs of real noisy images and corresponding ground truth images that were captured with consumer grade cameras of differing sensor sizes. For each pair, a reference image is taken with the base ISO level while the noisy image is taken with higher ISO and appropriately adjusted exposure time. The reference image undergoes a careful post-processing entailing small camera shift adjustment, linear intensity scaling and removal of low-frequency bias. The post-processed image serves as ground truth for our denoising benchmark.</p>
</div>
<div class="admonition-more-information admonition">
<p class="admonition-title">More Information</p>
<p><a class="reference external" href="https://github.com/oneTaken/Awesome-Denoise">Awesome Denoise</a></p>
</div>
</section>
<section id="optical-flows">
<h3>Optical flows<a class="headerlink" href="#optical-flows" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/optical_flows.jpg" /></p>
<p><span id="id3">[<a class="reference internal" href="#id29" title="Christian Lagemann, Kai Lagemann, Sach Mukherjee, and Wolfgang Schr√∂der. Deep recurrent optical flow learning for particle image velocimetry data. Nature Machine Intelligence, 3(7):641‚Äì651, July 2021.">LLMSchroder21</a>]</span></p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="https://europe.naverlabs.com/research/computer-vision/proxy-virtual-worlds-vkitti-1/">Virtual KITTI</a> -Virtual KITTI is a photo-realistic synthetic video dataset designed to learn and evaluate computer vision models for several video understanding tasks: object detection and multi-object tracking, scene-level and instance-level semantic segmentation, optical flow, and depth estimation.</p>
<p>Virtual KITTI contains 50 high-resolution monocular videos (21,260 frames) generated from five different virtual worlds in urban settings under different imaging and weather conditions. These worlds were created using the Unity game engine and a novel real-to-virtual cloning method. These photo-realistic synthetic videos are automatically, exactly, and fully annotated for 2D and 3D multi-object tracking and at the pixel level with category, instance, flow, and depth labels</p>
</div>
</section>
<section id="human-pose-estimation">
<h3>Human Pose Estimation<a class="headerlink" href="#human-pose-estimation" title="Permalink to this headline">#</a></h3>
<p>Predicting the pose of a human in an image</p>
<p><img alt="" src="../_images/pose_estimation_figure.png" /></p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="http://human-pose.mpi-inf.mpg.de/">MPII Human Pose Dataset</a> - MPII Human Pose dataset is a state of the art benchmark for evaluation of articulated human pose estimation. The dataset includes around 25K images containing over 40K people with annotated body joints. The images were systematically collected using an established taxonomy of every day human activities. Overall the dataset covers 410 human activities and each image is provided with an activity label. Each image was extracted from a YouTube video and provided with preceding and following un-annotated frames. In addition, for the test set we obtained richer annotations including body part occlusions and 3D torso and head orientations.</p>
<p>Following the best practices for the performance evaluation benchmarks in the literature we withhold the test annotations to prevent overfitting and tuning on the test set. We are working on an automatic evaluation server and performance analysis tools based on rich test set annotations.</p>
</div>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1911.10529">Simple Pose: Rethinking and Improving a Bottom-up Approach for Multi-Person Pose Estimation</a> -<a class="reference external" href="https://github.com/jialee93/Improved-Body-Parts">[CODE]</a> - Jia Li, Wen Su, Zengfu Wang (AAAI2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2002.06429">An End-to-End Framework for Unsupervised Pose Estimation of Occluded Pedestrians</a>   - Sudip Das, Perla Sai Raj Kishore, Ujjwal Bhattacharya (Arxiv 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2003.00080">Transferring Dense Pose to Proximal Animal Classes</a> - <a class="reference external" href="https://asanakoy.github.io/densepose-evolution/">[CODE]</a>  - Artsiom Sanakoyeu, Vasil Khalidov, Maureen S. McCarthy, Andrea Vedaldi, Natalia Neverova (CVPR 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2003.10506">Peeking into occluded joints: A novel framework for crowd pose estimation</a>   - ingteng Qiu, Xuanye Zhang, Yanran Li, Guanbin Li, Xiaojun Wu, Zixiang Xiong, Xiaoguang Han, Shuguang Cui (Arxiv 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2004.03234">Motion-supervised Co-Part Segmentation</a>   - Aliaksandr Siarohin*, Subhankar Roy*, St√©phane Lathuili√®re, Sergey Tulyakov, Elisa Ricci, Nicu Sebe (Arxiv 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2004.08154">Detailed 2D-3D Joint Representation for Human-Object Interaction</a> - <a class="reference external" href="https://github.com/DirtyHarryLYL/DJ-RN">[CODE]</a>  - Yong-Lu Li, Xinpeng Liu, Han Lu, Shiyi Wang, Junqi Liu, Jiefeng Li, Cewu Lu (CVPR 2020)</p></li>
<li><p><a class="reference external" href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Distribution-Aware_Coordinate_Representation_for_Human_Pose_Estimation_CVPR_2020_paper.pdf">Distribution Aware Coordinate Representation for Human Pose Estimation</a> - <a class="reference external" href="https://github.com/ilovepose/DarkPose">[CODE]</a>  - Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, Ce Zhu (CVPR 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2004.10362">Yoga-82: A New Dataset for Fine-grained Classification of Human Poses</a> - <a class="reference external" href="https://sites.google.com/view/yoga-82/home">[Data]</a>  - Manisha Verma, Sudhakar Kumawat, Yuta Nakashima, Shanmuganathan Raman (CVPRW 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2004.12652">Self-supervised Keypoint Correspondences for Multi-Person Pose Estimation and Tracking in Videos
</a>   - Rafi Umer, Andreas Doering, Bastian Leibe, Juergen Gall (Arxiv 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2006.15190">Making DensePose fast and light</a>   (Arxiv 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/2007.11864">Differentiable Hierarchical Graph Grouping for Multi-Person Pose Estimation</a> - Sheng Jin, Wentao Liu, Enze Xie, Wenhai Wang, Chen Qian, Wanli Ouyang, Ping Luo (ECCV 2020)</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2007.11858">Whole-Body Human Pose Estimation in the Wild</a> - <a class="reference external" href="https://github.com/jin-s13/COCO-WholeBody">[Data]</a>  - Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo (ECCV 2020)</p></li>
</ul>
</section>
<section id="d-object-pose-estimation">
<h3>6D Object Pose Estimation<a class="headerlink" href="#d-object-pose-estimation" title="Permalink to this headline">#</a></h3>
<p>Models to determine the location and orientation of objects from an image important for robotics</p>
<p><img alt="" src="../_images/centersnap_reconstruction_6d" /></p>
<p><span id="id4">[<a class="reference internal" href="#id28" title="Muhammad Zubair Irshad, Thomas Kollar, Michael Laskey, Kevin Stone, and Zsolt Kira. CenterSnap: Single-Shot Multi-Object 3D shape reconstruction and categorical 6D pose and size estimation. 2022.">IKL+22</a>]</span></p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="https://hci.iwr.uni-heidelberg.de/vislearn/iccv2015-occlusion-challenge/#Dataset">ICCV2015 Occluded Object Challenge</a> - The purpose of this challenge is to compare different methods for object pose estimation in a realistic setting featuring heavy occlusion. Our dataset includes eight objects in a cluttered scene. Given a RGB-D image the method has to estimate the position and orientation (a total of six degrees of freedom) of each object. You can participate by applying your method to our data and submitting your results. We will evaluate submitted results according to multiple metrics and display the scores for comparison.</p>
</div>
</section>
</section>
<section id="natural-language-processing">
<h2>Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="Permalink to this headline">#</a></h2>
<p>a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of ‚Äúunderstanding‚Äù the contents of documents, including the contextual nuances of the language within them.</p>
<p><em>Source Wikipedia</em></p>
<section id="extracting-knowledge-from-text-corpus">
<h3>Extracting Knowledge from Text Corpus<a class="headerlink" href="#extracting-knowledge-from-text-corpus" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/Materials_Science_NLP.webp" /></p>
<p><span id="id5">[<a class="reference internal" href="#id27" title="Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571(7763):95‚Äì98, July 2019.">TDW+19</a>]</span></p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="https://gluebenchmark.com/">GLUE (General Language Understanding Evaluation benchmark)</a> - General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding tasks, including single-sentence tasks CoLA and SST-2, similarity and paraphrasing tasks MRPC, STS-B and QQP, and natural language inference tasks MNLI, QNLI, RTE and WNLI.</p>
<p><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD (Stanford Question Answering Dataset)</a> - The Stanford Question Answering Dataset (SQuAD) is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers of questions can be any sequence of tokens in the given text. Because the questions and answers are produced by humans through crowdsourcing, it is more diverse than some other question-answering datasets. SQuAD 1.1 contains 107,785 question-answer pairs on 536 articles. SQuAD2.0 (open-domain SQuAD, SQuAD-Open), the latest version, combines the 100,000 questions in SQuAD1.1 with over 50,000 un-answerable questions written adversarially by crowdworkers in forms that are similar to the answerable ones.</p>
<p><a class="reference external" href="https://nlp.stanford.edu/sentiment">SST (Stanford Sentiment Treebank)</a> - The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.</p>
<p>Each phrase is labelled as either negative, somewhat negative, neutral, somewhat positive or positive. The corpus with all 5 labels is referred to as SST-5 or SST fine-grained. Binary classification experiments on full sentences (negative or somewhat negative vs somewhat positive or positive with neutral sentences discarded) refer to the dataset as SST-2 or SST binary.</p>
</div>
</section>
<section id="speech-recognition">
<h3>Speech Recognition<a class="headerlink" href="#speech-recognition" title="Permalink to this headline">#</a></h3>
<p>Learning context representations and information from audio files</p>
<p><img alt="" src="../_images/wav2vec2.png" /></p>
<p><span id="id6">[<a class="reference internal" href="#id26">BaevskiZhouMohamedothers</a>]</span></p>
<div class="admonition-benchmark-datasets admonition">
<p class="admonition-title">Benchmark Datasets</p>
<p><a class="reference external" href="http://www.openslr.org/12">LibriSpeech</a> - The LibriSpeech corpus is a collection of approximately 1,000 hours of audiobooks that are a part of the LibriVox project. Most of the audiobooks come from the Project Gutenberg. The training data is split into 3 partitions of 100hr, 360hr, and 500hr sets while the dev and test data are split into the ‚Äôclean‚Äô and ‚Äôother‚Äô categories, respectively, depending upon how well or challening Automatic Speech Recognition systems would perform against. Each of the dev and test sets is around 5hr in audio length. This corpus also provides the n-gram language models and the corresponding texts excerpted from the Project Gutenberg books, which contain 803M tokens and 977K unique words.</p>
<p><a class="reference external" href="https://research.google.com/audioset/index.html">AudioSet</a> - Audioset is an audio event dataset, which consists of over 2M human-annotated 10-second video clips. These clips are collected from YouTube, therefore many of which are in poor-quality and contain multiple sound-sources. A hierarchical ontology of 632 event classes is employed to annotate these data, which means that the same sound could be annotated as different labels. For example, the sound of barking is annotated as Animal, Pets, and Dog. All the videos are split into Evaluation/Balanced-Train/Unbalanced-Train set.</p>
</div>
</section>
</section>
<section id="generative-models">
<h2>Generative Models<a class="headerlink" href="#generative-models" title="Permalink to this headline">#</a></h2>
<p>Generative models generate new examples from a distribution</p>
<p><img alt="" src="../_images/generative_v_discriminative.png" /></p>
<p>In deep learning these models are called Generative Adversarial Networks (GANs) <span class="math notranslate nohighlight">\(\rightarrow\)</span> there are a ton of cool applications</p>
<section id="generating-anime-characters">
<h3>Generating Anime Characters<a class="headerlink" href="#generating-anime-characters" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/Generated_anime.jpg" /></p>
<p><span id="id7">[<a class="reference internal" href="#id19">JZL+17</a>]</span></p>
</section>
<section id="generating-images-from-text">
<h3>Generating Images from Text<a class="headerlink" href="#generating-images-from-text" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/text_to_image.jpg" /></p>
<p><span id="id8">[<a class="reference internal" href="#id18" title="Ayushman Dash, John Cristian Borges Gamboa, Sheraz Ahmed, Marcus Liwicki, and Muhammad Zeshan Afzal. TAC-GAN - text conditioned auxiliary classifier generative adversarial network. arXiv, March 2017. arXiv:1703.06412.">DGA+17</a>]</span></p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">IFrame</span>
<span class="n">IFrame</span><span class="p">(</span><span class="s1">&#39;http://gaugan.org/gaugan2/&#39;</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">2200</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">1200</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
<iframe
    width="2200"
    height="1200"
    src="http://gaugan.org/gaugan2/"
    frameborder="0"
    allowfullscreen

></iframe>
</div></div>
</div>
</section>
<section id="image-to-image-translation">
<h3>Image-to-Image Translation<a class="headerlink" href="#image-to-image-translation" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/cyclegans.jpeg" /></p>
<p><span id="id9">[<a class="reference internal" href="#id17" title="Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision, 2223‚Äì2232. openaccess.thecvf.com, 2017.">ZPIE17</a>]</span></p>
</section>
<section id="deepfakes">
<h3>DeepFakes<a class="headerlink" href="#deepfakes" title="Permalink to this headline">#</a></h3>
<p>Synthetic media where a person is replaced by the likeness (image and voice of another person)</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="s1">&#39;&lt;iframe width=&quot;602&quot; height=&quot;339&quot; src=&quot;https://www.youtube.com/embed/cQ54GDm1eL0&quot; title=&quot;You Won‚Äôt Believe What Obama Says In This Video! üòâ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/homebrew/Caskroom/miniforge/base/envs/jupyterbook/lib/python3.10/site-packages/IPython/core/display.py:419: UserWarning: Consider using IPython.display.IFrame instead
  warnings.warn(&quot;Consider using IPython.display.IFrame instead&quot;)
</pre></div>
</div>
<div class="output text_html"><iframe width="602" height="339" src="https://www.youtube.com/embed/cQ54GDm1eL0" title="You Won‚Äôt Believe What Obama Says In This Video! üòâ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div>
</div>
<p><img alt="" src="../_images/Faceswap.jpeg" /></p>
<p>from <a class="reference external" href="https://github.com/shaoanlu/faceswap-GAN">https://github.com/shaoanlu/faceswap-GAN</a></p>
</section>
</section>
<section id="physics-informed-and-physics-constrained-machine-learning">
<h2>Physics Informed and Physics Constrained Machine Learning<a class="headerlink" href="#physics-informed-and-physics-constrained-machine-learning" title="Permalink to this headline">#</a></h2>
<section id="physics-informed-neural-networks-pinns">
<h3>Physics-Informed Neural Networks (PINNs)<a class="headerlink" href="#physics-informed-neural-networks-pinns" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../_images/PINNS.webp" /></p>
<p><span id="id10">[<a class="reference internal" href="#id16" title="Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informed neural networks (PINNs) for fluid mechanics: a review. Acta Mech. Sin., January 2022.">CMW+22</a>]</span></p>
</section>
<section id="accelerated-fitting-using-physics-constrained-neural-networks">
<h3>Accelerated Fitting Using Physics-Constrained Neural Networks<a class="headerlink" href="#accelerated-fitting-using-physics-constrained-neural-networks" title="Permalink to this headline">#</a></h3>
<p>As long as the empirical functions are differentiable you can train a model to predict the parameters using the empirical function as a decoder.</p>
<section id="model-architecture">
<h4>Model Architecture<a class="headerlink" href="#model-architecture" title="Permalink to this headline">#</a></h4>
<p><img alt="" src="../_images/AE_image.svg" /></p>
</section>
<section id="fit-results">
<h4>Fit Results<a class="headerlink" href="#fit-results" title="Permalink to this headline">#</a></h4>
<p><img alt="" src="../_images/Fit_results.svg" /></p>
</section>
</section>
<section id="learning-underlying-governing-equations">
<h3>Learning Underlying Governing Equations<a class="headerlink" href="#learning-underlying-governing-equations" title="Permalink to this headline">#</a></h3>
<p>There are ways to take raw data and candidate functions and learn underlying governing equations using sparse identification</p>
<p><img alt="" src="../_images/lorenz.jpeg" /></p>
<p><span id="id11">[<a class="reference internal" href="#id15" title="Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences, 113(15):3932‚Äì3937, 2016.">BPK16</a>]</span></p>
</section>
</section>
<section id="reinforcement-learning">
<h2>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">#</a></h2>
<section id="playing-mario">
<h3>Playing Mario<a class="headerlink" href="#playing-mario" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="s1">&#39;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/qv6UVOQ0F44&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><iframe width="560" height="315" src="https://www.youtube.com/embed/qv6UVOQ0F44" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div>
</div>
</section>
<section id="much-more-complex-games">
<h3>Much More Complex Games<a class="headerlink" href="#much-more-complex-games" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="s1">&#39;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/UuhECwm31dM&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><iframe width="560" height="315" src="https://www.youtube.com/embed/UuhECwm31dM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div>
</div>
</section>
<section id="physical-object-manipulation-or-fine-motor-skills">
<h3>Physical Object Manipulation or Fine Motor Skills<a class="headerlink" href="#physical-object-manipulation-or-fine-motor-skills" title="Permalink to this headline">#</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">HTML</span><span class="p">(</span><span class="s1">&#39;&lt;iframe width=&quot;1268&quot; height=&quot;713&quot; src=&quot;https://www.youtube.com/embed/x4O8pojMF0w&quot; title=&quot;Solving Rubik‚Äôs Cube with a Robot Hand&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen&gt;&lt;/iframe&gt;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><iframe width="1268" height="713" src="https://www.youtube.com/embed/x4O8pojMF0w" title="Solving Rubik‚Äôs Cube with a Robot Hand" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div>
</div>
</section>
</section>
<section id="take-away-messages">
<h2>Take Away Messages:<a class="headerlink" href="#take-away-messages" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Data analysis and machine learning can be used in a variety of complex tasks</p></li>
</ul>
<ul class="simple">
<li><p>There are a variety of different implementations and methods for in machine learning</p></li>
</ul>
<ul class="simple">
<li><p>We have only just scratched the surface in how machine learning can be applied, this is a very exciting time</p></li>
</ul>
<ul class="simple">
<li><p>In this class we will prepare you to be a machine learning practitioner, and use machine learning for applications in science and manufacturing</p></li>
</ul>
<ul class="simple">
<li><p>Machine learning is a rapidly growing field. You could take a full course in each of these areas</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id12">
<dl class="citation">
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id11">BPK16</a></span></dt>
<dd><p>Steven¬†L Brunton, Joshua¬†L Proctor, and J¬†Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. <em>Proceedings of the National Academy of Sciences</em>, 113(15):3932‚Äì3937, 2016.</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id10">CMW+22</a></span></dt>
<dd><p>Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George¬†Em Karniadakis. Physics-informed neural networks (PINNs) for fluid mechanics: a review. <em>Acta Mech. Sin.</em>, January 2022.</p>
</dd>
<dt class="label" id="id18"><span class="brackets"><a class="fn-backref" href="#id8">DGA+17</a></span></dt>
<dd><p>Ayushman Dash, John Cristian¬†Borges Gamboa, Sheraz Ahmed, Marcus Liwicki, and Muhammad¬†Zeshan Afzal. TAC-GAN - text conditioned auxiliary classifier generative adversarial network. <em>arXiv</em>, March 2017. <a class="reference external" href="https://arxiv.org/abs/1703.06412">arXiv:1703.06412</a>.</p>
</dd>
<dt class="label" id="id20"><span class="brackets"><a class="fn-backref" href="#id1">DLT16</a></span></dt>
<dd><p>Chao Dong, Chen¬†Change Loy, and Xiaoou Tang. Accelerating the Super-Resolution convolutional neural network. <em>Computer Vision and Pattern Recognition</em>, August 2016. <a class="reference external" href="https://arxiv.org/abs/1608.00367">arXiv:1608.00367</a>.</p>
</dd>
<dt class="label" id="id28"><span class="brackets"><a class="fn-backref" href="#id4">IKL+22</a></span></dt>
<dd><p>Muhammad¬†Zubair Irshad, Thomas Kollar, Michael Laskey, Kevin Stone, and Zsolt Kira. CenterSnap: Single-Shot Multi-Object 3D shape reconstruction and categorical 6D pose and size estimation. 2022.</p>
</dd>
<dt class="label" id="id19"><span class="brackets"><a class="fn-backref" href="#id7">JZL+17</a></span></dt>
<dd><p><strong>missing journal in Jin2017-gy</strong></p>
</dd>
<dt class="label" id="id29"><span class="brackets"><a class="fn-backref" href="#id3">LLMSchroder21</a></span></dt>
<dd><p>Christian Lagemann, Kai Lagemann, Sach Mukherjee, and Wolfgang Schr√∂der. Deep recurrent optical flow learning for particle image velocimetry data. <em>Nature Machine Intelligence</em>, 3(7):641‚Äì651, July 2021.</p>
</dd>
<dt class="label" id="id27"><span class="brackets"><a class="fn-backref" href="#id5">TDW+19</a></span></dt>
<dd><p>Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin¬†A Persson, Gerbrand Ceder, and Anubhav Jain. Unsupervised word embeddings capture latent knowledge from materials science literature. <em>Nature</em>, 571(7763):95‚Äì98, July 2019.</p>
</dd>
<dt class="label" id="id30"><span class="brackets"><a class="fn-backref" href="#id2">XHC+20</a></span></dt>
<dd><p>Jun Xu, Yuan Huang, Ming-Ming Cheng, Li¬†Liu, Fan Zhu, Zhou Xu, and Ling Shao. Noisy-As-Clean: learning self-supervised denoising from corrupted image. <em>IEEE Trans. Image Process.</em>, September 2020.</p>
</dd>
<dt class="label" id="id17"><span class="brackets"><a class="fn-backref" href="#id9">ZPIE17</a></span></dt>
<dd><p>Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei¬†A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In <em>Proceedings of the IEEE International Conference on Computer Vision</em>, 2223‚Äì2232. openaccess.thecvf.com, 2017.</p>
</dd>
<dt class="label" id="id26"><span class="brackets"><a class="fn-backref" href="#id6">BaevskiZhouMohamedothers</a></span></dt>
<dd><p><strong>missing year in Baevski_undated-gm</strong></p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "jagar2/Fall_2022_MEM_T680Data_Analysis_and_Machine_Learning",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./What_you_can_do_with_machine_learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Joshua C. Agar<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>